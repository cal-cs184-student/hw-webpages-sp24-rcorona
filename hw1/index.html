<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184/284A Rasterizer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184/284A: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Homework 1: Rasterizer</h1>
<h2 align="middle">Rodolfo Corona Rodriguez (3034203306) </h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p>Give a high-level overview of what you implemented in this homework Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.</p>

<h2 align="middle">Section I: Rasterization</h2>

<h3 align="middle">Part 1: Rasterizing single-color triangles</h3>
  
  <p>
  My algorithm uses a couple of helper functions, which I will describe first, I will then conclude with a high level overview of the algorithm.
  </p>
  <p><br>
  <p><b>Helper Functions:</b></p>
  <i>line_test</i>: This helper takes as input a pair of points ((x_0, y_0), (x_1, y_1)) defining a line and a third point (x,y ) to serve as a query. 
  It performs a line test by first defining two vectors: <br>V = (x - x0, y - y0), i.e. the vector from the first line point to the query point, and 
  <br>N = (-(y1 - y0), x1 - x0), i.e. a vector perpendicular to the line. 
  Having defined these two vectors, the function returns the result of their dot product, which will be positive
  if the query point lies on the positive half-plane of the line, 0 if it lies directly on the line, or negative if it lies in the negative half-plane. 
  </p>
  <p>
  <i>inside</i>: Given three triangle points and a query point, this helper function determines if the query lies in the triangle. 
  As a first step, the function checks which direction denotes the inside of the triangle if we follow the vertexi winding order of P0 --> P1, P1 --> P2, and P2 --> P0. 
  The direction is determined by performing a line test using P0 and P1 as the two line points and P2 as the query point. 
  If the direction line test is positive, then points in the triangle will lie on the positive half-plane of each triangle edge (or, conversely, on the negative half-plane if the line test is negative). 
  Therefore, a <i>direction</i> variable is set to +1 or -1 depending on the result of th direction line test. 
  Having determined the correct direction, the function then performs a line test for the query pixel on each edge (determined by each pair of triangle vertices) and multiplying it by the direction variable.
  If all line tests are >= 0 then that means the point lies on the correct side of each edge and the function returns True (and otherwise returnes False). 
  </p>
  <p>
  <b>High-level overview: </b> First, the algorithm determines the bounding box of the triangle by finding the maximum and minimum x and y coordinates in \{x0, x1, x2\} and \{y0, y1, y2\}. 
  Having determined the bounding box, a nested for loop (one for x and one for y coordinates) from the minimum (integer) x,y to the maximum in increments of 1 is traversed. 
  For each pair of (x,y) query points in the loop, the algorithm performs an <i>inside</i> test on the sample point (x + 0.5, y + 0.5) to determine if the point lies in the triangle or not.  
  If the sample point lies within the triangle then the pixel (x,y) is filled. 
  </p>
  <p>
  <b>Difficult Part:</b> I had the most trouble with making the algorithm robust to triangle vertex winding ordering. At first my algorithm assumed that all 3 line tests being non-negative 
  <i>always</i> means that a point lies in the triangle, however this is only true if P2 lies in the positive half-plane of the edge P0-->P1. If P2 lies on the negative half-plane 
  then the line tests must all be non-positive instead. Adding this check resolved the issue. 
  </p>
  <p>Below is a render of <i>basic/test4.svg</i> with the pixel inspector zoomed into the left corner of the red triangle. As may be observed, there is a discontinuity between some pixels on the tip of the corner and the rest of the triangle due to aliasing.</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task1_img.png" align="middle" width="400px"/>
        <figcaption align="middle">Render of <i>basic/test4.svg</i> with the pixel inspector zoomed into the left corner of the red triangle.</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 2: Antialiasing triangles</h3>
  
<p>
<b>Data Structures:</b> My algorithm primarily uses the <i>sample_buffer</i> to maintain a super-sampled image during rasterization, the size of this buffer will be the (width * height * sample_rate). <br>
  Additionally, I maintain a <i>sqrt_sample_rate</i> variable which is only computed once per sample rate change such that we don't have to perform a sqrt operation each time we need it. 
</p>

<p>
<b>Algorithm Walkthrough</b>: When rasterizing a triangle, the algorithm first scales the coordinates of the triangle by <i>sqrt_sample_rate</i> in order to get it at the scale of the now super-sampled sample buffer.<br>
  Following this, rasterization of the triangle follows as usual, with the rasterizer looping over the pixels in the bounding box of the (now scaled) triangle. <br>
  After all the triangles are rasterized and <i>resolve_to_frame_buffer</i> is called, the algorithm will compute the color value of each frame buffer pixel by averaging the values of all sample_buffer locations pertaining to that pixel.<br>
  During frame buffer resolution process, the sample buffer coordinates for pixel (x,y) will be ((j * width * sqrt_sample_rate), (y * width * sample_rate)) for i,j in [0, sqrt_sample_rate]. 
</p>

  <p>
  <b>Why is super-sampling useful?:</b> Super-sampling is useful because it scales the sampling frequency by sqrt_sample_rate in each axis, reducing aliasing. <br>
    Increasing the sample frequency combats aliasing because it raises the Nyquist frequency (the rate which signal frequencies must stay below in order to avoid aliasing) for the signal, which is going to be half the sample rate. <br>
    In other words, because we are increasing the frequency of the samples, we are able to capture faster ocurring changes in the image. 
  </p>
  <p>
  <b>What modifications did I make?:</b> Firstly, the sample buffer data structure was dynamically updated to be able to fit the appropriate number of samples. <br>
    In both the <i>set_sample_rate</i> and <i>set_framebuffer_target</i> methods the sample buffer is resized to (width * height * sample_rate) to accomodate the samples. <br>
    The <i>fill_pixel</i> method was modified to access the sample buffer at the (y * width * sqrt_sample_rate + x) location in order to accomodate the scaled coordinates. <br>
    Lastly, the <i>rasterize_triangle</i> and <i>resolve_to_frame_buffer</i> methods were modified as described above. 
  </p>
  <p>
  Below is a side-by-side comparison of sample rates of 1, 4, and 16 with the pixel inspecto zoomed over the left corner of the red triangle in <i>basic/test4.svg</i>.<br>

  </p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task2_1.png" align="middle" width="400px"/>
        <figcaption align="middle">Sampling rate of 1.</figcaption>
      </td>
      <td>
        <img src="images/task2_4.png" align="middle" width="400px"/>
        <figcaption align="middle">Sampling rate of 4.</figcaption>
      </td>
      <td>
        <img src="images/task2_16.png" align="middle" width="400px"/>
        <figcaption align="middle">Sampling rate of 16.</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  As may be observed, the corner becomes smoother as the sampling rate increases. <br>
  To have a smoother image, the ideal color (assuming there is only one triangle + the background in a pixel) would be the color of the triangle weighted by the proportion of the pixel which the triangle is filling. <br>
  However, solving this analyitically would be much more expensive, and so samples are taken to approximate this proportion. <br>
  Higher sampling rates yield better approximations of the true proportion, and thus higher sample rates yield smoother images with less aliasing. 
</p>

<h3 align="middle">Part 3: Transforms</h3>

  <p>
For my cubeman version, I wanted to have cubeman look like they were jumping and raising their hands up in the air.<br>
To accomplish this, the legs were rotated by 45 degrees each and translated a little higher to get the look right. <br>
Similarly, the arms were rotated by 90 degrees and translated up to look like they were being raised up all the way.<br>
Lastly, the legs were colored blue and the arms were colored green. 
</p>
    
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task3_myrobot.png" align="middle" width="400px"/>
        <figcaption align="middle">Cubeman jumping and raising their hands up in the air.</figcaption>
      </td>
    </tr>
  </table>
</div>

<h2 align="middle">Section II: Sampling</h2>

<h3 align="middle">Part 4: Barycentric coordinates</h3>

Barycentric coordinates are the coordinates for a point in relation to the vertices of a triangle. <br>
Each of the three coordinates, (alpha, beta, gamma), represent the relative weight the proximity of each vertex has to a given query point. 
In other words, the barycentric coordinates represent the spatial weighted sum of the three vertices which would result in the coordinates 
of the query point. <br>
Because these weights vary smoothly, as one moves the query point across the triangle, then these same weights can be used to smoothly interpolate
any other value (e.g. color, surface normals, etc.) across the triangle. 
For example, the image below uses barycentric coordinates to smoothly interpolate the color values of a red, green, and blue cornered triangle across the
body of the triangle. 

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4_interp_triangle.png" align="middle" width="400px"/>
        <figcaption align="middle">Cubeman jumping and raising their hands up in the air.</figcaption>
      </td>
    </tr>
  </table>
</div>

Additionally, here is an image of a full color gradient wheel rasterized using barycentric coordinate interpolation. 

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/task4_gradient.png" align="middle" width="400px"/>
        <figcaption align="middle">Cubeman jumping and raising their hands up in the air.</figcaption>
      </td>
    </tr>
  </table>
</div>
  
<h3 align="middle">Part 5: "Pixel sampling" for texture mapping</h3>

  <p>
<b>Pixel Sampling Definition:</b> Pixel sampling from textures is the process through which color samples from an image, known as the texture, are mapped to screen space during the rasterization process. <br>
  To do this, a bijective function is defined which maps from coordinates in screen space (or coordinates in a mesh) to coordinates within the texture image (referred to as the (u,v) coordinates of the point). <br>
  The rate of change of this mapping function need not be constant, i.e. du/dx, du/dy, dv/dx, and dv/dy are not necessarily across screen space. <br>
    Coordinates (u,v) lie in the continuous range [0,1], and thus when converting to discrete texel coordinates in the texture image, they must be scaled by the width and height of the texture image and then discretized int an integer coordinate. 
  </p>
<p>
  <b>Implementation:</b> TODO
</p>
  <p>
    <b>Sampling Methods</b>
  </p>
  <p>
    <i>Nearest Neighbor Sampling:</i> Given a (u,v) in the texture space for a screen space coordinate (x,y), nearest neighbor sampling returns the texel (texture image pixel) pertaining to the nearest discrete coordinate in the texture image. <br>
    Assuming that sampling is performed based on distance to texel center (i.e. the distance to each texel is computed from (tx + 0.5, ty + 0.5)) then the nearest discrete texel coordinate for a continuous (u,v) is going to be the floor of each of those two values. 
  </p>
  <p>
    <i>Bilinear Sampling<:i/> Bilinear sampling computes the returned texture sample as the weighted average of the four nearest texels to a given (u,v) coordinate (with the weights being proportional to the distance). <br>
      The appropriate interpolation can be performed by a sequence of three lerp (linear interpolation) operations. <br>
      First, the two texels in each row are linearly interpolated into intermediate color values proportionally to their horizontal distance from the sample (u,v) point. <br>
      Next, the two intermediate values are linearly interpolated into a final value proprtionally to their vertical distance from the sample (u,v) point. The result of this third lerp is what is returned as the texture sample.  
  </p>

<h3 align="middle">Part 6: "Level sampling" with mipmaps for texture mapping</h3>



<h2 align="middle">Section III: Art Competition</h2>
<p>If you are not participating in the optional art competition, don't worry about this section!</p>

<h3 align="middle">Part 7: Draw something interesting!</h3>

</body>
</html>
